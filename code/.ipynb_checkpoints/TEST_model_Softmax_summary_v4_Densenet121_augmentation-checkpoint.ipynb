{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime                  # test + Uncertainty\n",
    "import pandas as pd              # test + Uncertainty\n",
    "import numpy as np               # test + Uncertainty\n",
    "import pickle5 as pickle                    # Uncertainty\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import test_model as tm          # test\n",
    "import test_uncertainty_Softmax_v2 as tuc   # Uncertainty\n",
    "from humanfriendly import format_timespan\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import performance\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from scipy.special import entr\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_pretrained_models as ppm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from configparser import ConfigParser\n",
    "import custom_dataset as cds\n",
    "import data_ops as do\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-24 15:17:12,334  train_ppt   INFO  self.device: cpu\n",
      "2021-05-24 15:17:12,517  train_ppt   INFO  model in use: densenet121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 24.05.2021_15.17.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_data_set: test\n",
      "self.img_dir: ../data/test/singles/\n",
      "annotations_file: ../data/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "config_path = '../config/config_densenet121_augmentation.ini'\n",
    "config = ConfigParser()\n",
    "config.read(config_path)\n",
    "l_pictures = []; l_outputs = []; l_labels = []\n",
    "\n",
    "root_dir = config.get('model', 'root_dir')\n",
    "data_dir = data_dir = config.get('data', 'data_dir')\n",
    "model_list = {config.get('model', 'name')} \n",
    "num_classes = config.getint('model', 'num_classes')\n",
    "batch_size = config.getint('model', 'batch_size')\n",
    "shuffle = config.getboolean('data', 'shuffle')\n",
    "t_data_set = config.get('test', 't_data_set')          # 'test' or 'val_test'\n",
    "single_channel = config.getboolean('model', 'single_channel')\n",
    "model_meta_csv = config.get('test', 'model_meta_csv') \n",
    "test_data_csv = f'{root_dir}data/test_data.csv'\n",
    "data_transform = config.get('data', 'transform')\n",
    "\n",
    "torch.manual_seed(config.getint('data', 'seed'))\n",
    "np.random.seed(config.getint('data', 'seed'))\n",
    "\n",
    "dops1 = do.dops(config)\n",
    "\n",
    "pre_model_test = ppm.pt_model(config)\n",
    "model_test, input_size = pre_model_test.initialize()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dops1.pick_data_transforms(data_transform, pre_model_test.input_size)\n",
    "model_test.load_state_dict(torch.load('../models/dir_densenet121/densenet121_11epochs_0.9566.weights', map_location=device))\n",
    "model_test.eval()\n",
    "\n",
    "if device == 'cpu':\n",
    "    model_test.cpu()\n",
    "\n",
    "test_dataset = cds.MyImageDataset(config, dops1, t_data_set = t_data_set)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "aug_tst = 0\n",
    "for pictures, inputs, _, labels in tqdm(test_dataloader):        \n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    for i in range(10):\n",
    "        single_input = inputs[:, i, ...]\n",
    "        outputs = model_test(single_input)\n",
    "        outputs = outputs.to('cpu')\n",
    "        outputs = outputs.detach().numpy()\n",
    "\n",
    "        l_pictures.extend(pictures)\n",
    "\n",
    "        l_outputs.extend(outputs)\n",
    "        l_labels.extend(labels)\n",
    "\n",
    "    if aug_tst == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2class(idx):\n",
    "    return index_to_class[index_to_class['index']==idx]['class'][idx]\n",
    "\n",
    "df_res = pd.DataFrame({'l_pictures': l_pictures, 'l_outputs': l_outputs, 'l_labels': l_labels})\n",
    "index_to_class = pd.read_csv('../data/index_to_class.csv')\n",
    "class_to_index = index_to_class.set_index('class')\n",
    "class_to_index.columns=['idx']\n",
    "df_res['pred'] = df_res['l_outputs'].apply(lambda x: idx2class(np.argmax(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_res\n",
    "len(df_res[df_res['pred']==df_res['l_labels']])/len(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiple_copies = True\n",
    "if multiple_copies:\n",
    "    # inputs.shape # torch.Size([64, 10, 3, 224, 224])\n",
    "    input_s = inputs[9,6,...]\n",
    "else:\n",
    "    input_s = inputs[9,...]\n",
    "input_s = input_s.permute(1, 2, 0)\n",
    "input_s = input_s.to('cpu')\n",
    "input_s = input_s.detach().numpy()\n",
    "# input_s.shape\n",
    "plt.imshow(input_s, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_pictures</th>\n",
       "      <th>l_outputs</th>\n",
       "      <th>l_labels</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.3600848, 3.0242553, -1.8687322, -4.158342, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.3600848, 3.0242553, -1.8687322, -4.158342, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.3600848, 3.0242553, -1.8687322, -4.158342, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.3600848, 3.0242553, -1.8687322, -4.158342, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.3600848, 3.0242553, -1.8687322, -4.158342, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.5167626, 3.1647336, -1.8651068, -3.6785884,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.5167626, 3.1647336, -1.8651068, -3.6785884,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.5167626, 3.1647336, -1.8651068, -3.6785884,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.5167626, 3.1647336, -1.8651068, -3.6785884,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>MA160865191500021.009-K_10_1_.png</td>\n",
       "      <td>[1.5167626, 3.1647336, -1.8651068, -3.6785884,...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            l_pictures  \\\n",
       "0    MA160865191500021.009-K_10_1_.png   \n",
       "64   MA160865191500021.009-K_10_1_.png   \n",
       "128  MA160865191500021.009-K_10_1_.png   \n",
       "192  MA160865191500021.009-K_10_1_.png   \n",
       "256  MA160865191500021.009-K_10_1_.png   \n",
       "320  MA160865191500021.009-K_10_1_.png   \n",
       "384  MA160865191500021.009-K_10_1_.png   \n",
       "448  MA160865191500021.009-K_10_1_.png   \n",
       "512  MA160865191500021.009-K_10_1_.png   \n",
       "576  MA160865191500021.009-K_10_1_.png   \n",
       "\n",
       "                                             l_outputs l_labels pred  \n",
       "0    [1.3600848, 3.0242553, -1.8687322, -4.158342, ...       10   10  \n",
       "64   [1.3600848, 3.0242553, -1.8687322, -4.158342, ...       10   10  \n",
       "128  [1.3600848, 3.0242553, -1.8687322, -4.158342, ...       10   10  \n",
       "192  [1.3600848, 3.0242553, -1.8687322, -4.158342, ...       10   10  \n",
       "256  [1.3600848, 3.0242553, -1.8687322, -4.158342, ...       10   10  \n",
       "320  [1.5167626, 3.1647336, -1.8651068, -3.6785884,...       10   10  \n",
       "384  [1.5167626, 3.1647336, -1.8651068, -3.6785884,...       10   10  \n",
       "448  [1.5167626, 3.1647336, -1.8651068, -3.6785884,...       10   10  \n",
       "512  [1.5167626, 3.1647336, -1.8651068, -3.6785884,...       10   10  \n",
       "576  [1.5167626, 3.1647336, -1.8651068, -3.6785884,...       10   10  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res[df_res['l_pictures'] == 'MA160865191500021.009-K_10_1_.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[:,0,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({'l_pictures': l_pictures, 'l_outputs': l_outputs, 'l_labels': l_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['l_outputs'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(input_s, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_s = inputs[9,6,...]\n",
    "input_s = input_s.permute(1, 2, 0)\n",
    "input_s = input_s.to('cpu')\n",
    "input_s = input_s.detach().numpy()\n",
    "# input_s.shape\n",
    "plt.imshow(input_s, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['pred'] = df_res['l_outputs'].apply(lambda x: np.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[df_res['l_pictures'] == 'MA160865191500021.009-K_10_1_.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_test = False\n",
    "start_date=datetime.datetime.now()\n",
    "print(start_date)\n",
    "if execute_test:        \n",
    "    #%run test_model --config ../config/config_squeezenet1_0.ini\n",
    "    %run test_model_augmentation --config ../config/config_densenet121_augmentation.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN TEST ON PICTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', -1)\n",
    "# idx=np.argmax(df_result_ensemble.iloc[0]['outputs_mean'])\n",
    "# df_result_ensemble.iloc[0]['outputs_mean'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tm1.df_result\n",
    "print(f'uncertainty running: {datetime.datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN UNCERTAINTY CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 500)\n",
    "# saving a DataFrame as csv saves lista (e.g. outputs_mean) as string which leads to conversion problems \n",
    "# while loading. so, it's better to save as a pickle file and load it back.\n",
    "# with open('../results/squeezenet1_0_11.05.2021_02.07.07_df_result_ensemble.pkl', 'rb') as fp:\n",
    "#      df_result_ensemble = pickle.load(fp)\n",
    "\n",
    "# df_result_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuc1 = tuc.test_uncertainty('../config/config_squeezenet1_0.ini')\n",
    "# tuc1 = tuc.test_uncertainty('../config/config_alexnet.ini')\n",
    "# tuc1 = tuc.test_uncertainty('../config/config_resnet18.ini')\n",
    "# use_sample_weights = [0,1,3,5,6]\n",
    "use_sample_weights = [-1]  # if -1 then use all weights in the given ***** result pkl ******\n",
    "tuc1 = tuc.test_uncertainty('../config/config_densenet121.ini', use_sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(len(tuc1.df_result_ensemble.loc[0,'outputs_all'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_class = pd.read_csv('../data/index_to_class.csv')\n",
    "# index_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_index = index_to_class.set_index('class')\n",
    "class_to_index.columns=['idx']\n",
    "# class_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_column = []\n",
    "if use_sample_weights == [-1]:\n",
    "    use_sample_weights = list(range(len(tuc1.df_result_ensemble.loc[0,'outputs_all'])))\n",
    "\n",
    "# accuracy per model\n",
    "for i, sample_model in enumerate(use_sample_weights):\n",
    "    tuc1.df_result_ensemble[f'pred_{sample_model}'] = \\\n",
    "        tuc1.df_result_ensemble.loc[:,'outputs_all'].apply(lambda x: np.argmax(x[i]))\n",
    "    lst_column.append(f'pred_{sample_model}')\n",
    "\n",
    "# accuracy per model\n",
    "n_models = len(use_sample_weights)\n",
    "count_correct = np.zeros(n_models)\n",
    "lst_incorrect = [[] for i in range(n_models)]\n",
    "    \n",
    "for pic in range(len(tuc1.df_result_ensemble)):\n",
    "    true_label = tuc1.df_result_ensemble['label'][pic]\n",
    "    for i, sample_model in enumerate(use_sample_weights):\n",
    "        #tuc1.index_to_class['class'][tuc1.df_result_ensemble[f'pred_{sample_model}'][pic]]\n",
    "        if tuc1.index_to_class['class'][tuc1.df_result_ensemble.loc[pic ,f'pred_{sample_model}']] == true_label:\n",
    "            count_correct[i] += 1\n",
    "        else:\n",
    "            lst_incorrect[i].append(pic)\n",
    "\n",
    "# count_correct/len(tuc1.df_result_ensemble)*100\n",
    "[round(x/len(tuc1.df_result_ensemble)*100,4) for x in count_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.iloc[:2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.run()\n",
    "df_results_eval = tuc1.df_uncertainty  # df_final\n",
    "df_final_summary = tuc1.df_final_summary\n",
    "\n",
    "df_results_eval\n",
    "#df_results_eval.loc[:0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuc1.df_result_ensemble.drop(columns=['preds','preds_count', 'preds_rate', 'Incorrect'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble['preds'] = \\\n",
    "[row for row in tuc1.df_result_ensemble[lst_column].values]\n",
    "\n",
    "tuc1.df_result_ensemble['preds_count'] = \\\n",
    "tuc1.df_result_ensemble['preds'].apply(lambda x: np.bincount(x))\n",
    "\n",
    "tuc1.df_result_ensemble['preds_rate'] = \\\n",
    "tuc1.df_result_ensemble['preds_count'].apply(lambda x: x[np.argmax(x)]/n_models)\n",
    "\n",
    "l_column = ['picture', 'model', 'label', 'preds','preds_count', 'preds_rate']+lst_column\n",
    "\n",
    "searchfor = list(df_results_eval[df_results_eval['actual_class']!=df_results_eval['best_pred']]['picture'])\n",
    "\n",
    "tuc1.df_result_ensemble['Incorrect']=tuc1.df_result_ensemble['picture'].apply(lambda x: \\\n",
    "                                                1 if any(i in x for i in searchfor) else 0)\n",
    "\n",
    "df_results_eval['Incorrect']=df_results_eval['picture'].apply(lambda x: \\\n",
    "                                                1 if any(i in x for i in searchfor) else 0)\n",
    "\n",
    "#tuc1.df_result_ensemble.iloc[:5, -8:]\n",
    "tuc1.df_result_ensemble.loc[:5, l_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # searchfor = ['MA160814197500042.017-K_9_0_.png', 'MA160814197500041.070-K_4_0_.png']\n",
    "# searchfor = list(df_results_eval[df_results_eval['actual_class']!=df_results_eval['best_pred']]['picture'])\n",
    "# # s[s.str.contains('|'.join(searchfor))]\n",
    "# # tuc1.df_result_ensemble[tuc1.df_result_ensemble['pred_6'].str.contains('|'.join(searchfor))]\n",
    "\n",
    "# tuc1.df_result_ensemble['Incorrect']=tuc1.df_result_ensemble['picture'].apply(lambda x: 1 if any(i in x for i in searchfor) else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuc1.df_result_ensemble.drop(columns=['preds','preds_count','preds_max', 'Incorrect'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.iloc[:2,-11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_column = []\n",
    "# for i in range(n_models):\n",
    "#     lst_column.append(f'pred_{i}')\n",
    "\n",
    "# lst_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_column = []\n",
    "# for i in range(n_models):\n",
    "#     lst_column.append(f'pred_{i}')\n",
    "# vertical variance\n",
    "\n",
    "l_th, l_FP, l_TN, l_FN, l_work_count, l_fail_count, l_work_rate, l_fail_rate = [],[],[],[],[],[],[],[] \n",
    "# th_rate = round(1/n_models*5+0.01, 2) # e.g 0.68\n",
    "\n",
    "# l_FP = len(tuc1.df_result_ensemble[(tuc1.df_result_ensemble['Incorrect']==1) & \\\n",
    "#                         (tuc1.df_result_ensemble['preds_rate']>=th_rate)].loc[:, lst_column])\n",
    "\n",
    "# l_TN = len(tuc1.df_result_ensemble[(tuc1.df_result_ensemble['Incorrect']==1) & \\\n",
    "#                         (tuc1.df_result_ensemble['preds_rate']<=th_rate)].loc[:, lst_column])\n",
    "\n",
    "# l_FN = len(tuc1.df_result_ensemble[(tuc1.df_result_ensemble['Incorrect']==0) & \\\n",
    "#                         (tuc1.df_result_ensemble['preds_rate']<=th_rate)].loc[:, lst_column])\n",
    "\n",
    "# work_load = l_FN+l_TN\n",
    "# fail_rate = l_FP\n",
    "\n",
    "# print(f'threshold={th_rate} .. l_FN: {l_FN} .. l_FN: {l_TN} .. l_FN: {l_FP}')\n",
    "# print(f'work_load(%): {round(work_load/len(tuc1.df_result_ensemble)*100, 2)} .. n_work_load: {work_load}')\n",
    "# print(f'fail_rate(%): {round(fail_rate/len(tuc1.df_result_ensemble)*100, 2)} .. fail_rate: {fail_rate}')\n",
    "\n",
    "for i in range(n_models):\n",
    "    th_rate = round(1/n_models*i+0.01, 2)\n",
    "    n_FP = len(tuc1.df_result_ensemble[(tuc1.df_result_ensemble['Incorrect']==1) & \\\n",
    "                        (tuc1.df_result_ensemble['preds_rate']>=th_rate)].loc[:, lst_column])\n",
    "\n",
    "    n_TN = len(tuc1.df_result_ensemble[(tuc1.df_result_ensemble['Incorrect']==1) & \\\n",
    "                            (tuc1.df_result_ensemble['preds_rate']<=th_rate)].loc[:, lst_column])\n",
    "\n",
    "    n_FN = len(tuc1.df_result_ensemble[(tuc1.df_result_ensemble['Incorrect']==0) & \\\n",
    "                            (tuc1.df_result_ensemble['preds_rate']<=th_rate)].loc[:, lst_column])\n",
    "\n",
    "    n_work_count = n_FN + n_TN\n",
    "    n_fail_count = n_FP\n",
    "    \n",
    "    l_th.append(th_rate)\n",
    "    l_FP.append(n_FP)\n",
    "    l_TN.append(n_TN)\n",
    "    l_FN.append(n_FN)\n",
    "    l_work_count.append(n_work_count)\n",
    "    l_fail_count.append(n_fail_count)\n",
    "    l_fail_rate.append(round(n_fail_count/len(tuc1.df_result_ensemble)*100, 2))\n",
    "    l_work_rate.append(round(n_work_count/len(tuc1.df_result_ensemble)*100, 2))\n",
    "    \n",
    "    df_ensemble_res = pd.DataFrame({'th': l_th, 'FP': l_FP, 'TN': l_TN, 'FN': l_FN, 'Work_Load': l_work_count, \n",
    "                      'Fail Rate': l_fail_count, 'Work_Load (%)': l_work_rate, 'Fail Rate (%)': l_fail_rate})\n",
    "\n",
    "df_ensemble_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flexible = df_ensemble_res[(df_ensemble_res['Work_Load (%)']<=5) & (df_ensemble_res['Fail Rate (%)']<=3)] \n",
    "lst_flexible = list(df_flexible['th'])\n",
    "lst_flexible_workload = list(df_flexible['Work_Load (%)'])\n",
    "print(f'{len(lst_flexible)} values from {min(lst_flexible)} to {max(lst_flexible)} having fail rate between 0.5 and 1.0')\n",
    "print(f'workload from {min(lst_flexible_workload)} to {max(lst_flexible_workload)}')\n",
    "\n",
    "print(3.753371-3.622185)\n",
    "print((3.753371-3.622185)/3.622185*100)\n",
    "\n",
    "df_flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble_res['Work_Load (%)']<=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter(tuc1.df_result_ensemble.loc[0, 'preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'threshold={th_rate} .. l_FN: {l_FN} .. l_TN: {l_TN} .. l_FP: {l_FP}')\n",
    "# print(f'work_load(%): {round(l_sum/len(tuc1.df_result_ensemble)*100, 2)} .. n_work_load: {work_load}')\n",
    "# print(f'fail_rate(%): {round(fail_rate/len(tuc1.df_result_ensemble)*100, 2)} .. fail_rate: {fail_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(142)/len(tuc1.df_result_ensemble)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1135+142)/len(tuc1.df_result_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_count = np.zeros((n_models, n_models))\n",
    "m_ratio = np.zeros((n_models, n_models))\n",
    "m_jaccard = np.zeros((n_models, n_models))  # (A&B)/(A+B)\n",
    "for m1 in range(n_models):    \n",
    "    for m2 in range(m1+1,n_models):\n",
    "        lst_diff=[]\n",
    "        lst_same=[]\n",
    "        for i in lst_incorrect[m1]:\n",
    "            if i in lst_incorrect[m2]:\n",
    "                lst_same.append(i)\n",
    "            else:\n",
    "                lst_diff.append(i)\n",
    "                \n",
    "        m_count[m1,m2]=len(lst_diff)\n",
    "        m_ratio[m1,m2]=round((len(lst_diff)/len(lst_incorrect[m1])),2)\n",
    "        total_incorrect = set(lst_incorrect[m1] + lst_incorrect[m2])\n",
    "        m_jaccard[m1,m2]=round(len(lst_same)/len(total_incorrect),2)   \n",
    "\n",
    "print('Diff_count matrix for incorrect labels between different models')\n",
    "print(m_count)\n",
    "\n",
    "print('\\nDiff_ratio matrix for incorrect labels between different models')\n",
    "print(m_ratio)\n",
    "\n",
    "print('\\nJaccard similarity matrix for incorrect labels between different models')\n",
    "print(m_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tuc1.df_result_ensemble)):\n",
    "#     tuc1.df_result_ensemble['outputs_all'][i] = tuc1.df_result_ensemble['outputs_all'][i][lst_max5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After picking the best-5 accuracy-models, the ensemble accuracy dropped to 96.9827 from 97.092\n",
    "BUT\n",
    "CERTAINTY results getting BETTER\n",
    "'''\n",
    "# lst_acc = count_correct/len(tuc1.df_result_ensemble)*100\n",
    "# lst_acc_sort = lst_acc.copy()\n",
    "# lst_acc_sort.sort()\n",
    "# lst_acc_sort[2]\n",
    "# lst_max5 = [i for i in range(len(lst_acc)) if lst_acc[i]>=lst_acc_sort[2]]\n",
    "# lst_max5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_results_eval[df_results_eval['actual_class']!=df_results_eval['best_pred']]['picture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime.datetime.now()\n",
    "t_delta = end_date - start_date\n",
    "\n",
    "print(f'ALL done: {end_date} .. total calculation time: {format_timespan(t_delta.seconds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_test(plt, title='Softmax Score', metric_name='best_score',step_size=0.01, higher_better=True, is_norm=False):\n",
    "    plus_minus = 5\n",
    "    max_val = round(max(df_results_eval[metric_name]), 6)\n",
    "    min_val = round(min(df_results_eval[metric_name]), 6)\n",
    "    mean_val = round(np.mean(df_results_eval[metric_name]), 6)\n",
    "    var_val = round(np.var(df_results_eval[metric_name]), 6)\n",
    "    \n",
    "    norm_name = f'{metric_name}_Norm'\n",
    "    if is_norm and norm_name not in df_results_eval.columns:\n",
    "        df_results_eval[norm_name]=df_results_eval[metric_name].apply(lambda x: (x-min_score)/\n",
    "                                                                      (max_score-min_score)*100)\n",
    "    \n",
    "    print(f'max: {max_val} .. min: {min_val} .. mean: {mean_val} .. var: {var_val}')\n",
    "    \n",
    "    x=[];\n",
    "    TP_temp, FP_temp, TN_temp, FN_temp = 0, 0, 0, 0\n",
    "    TP, FP, TN, FN, TPR, TNR, FPR, FDR, NPV = [], [], [], [], [], [], [], [], []\n",
    "    fail_rate_temp, work_load_temp, fail_rate, work_load = 0, 0, [], []\n",
    "    #for val in tqdm(np.arange(min_val+0.0001, max_val + step_size, step_size)):\n",
    "    for val in tqdm(np.arange(0, max_val + 2*step_size, step_size)):\n",
    "        \n",
    "        # All Correct predictions\n",
    "        len_Correct = len(df_results_eval[df_results_eval['actual_class'] == df_results_eval['best_pred']])\n",
    "        # All INcorrect predictions\n",
    "        len_Incorrect = len(df_results_eval[df_results_eval['actual_class'] != df_results_eval['best_pred']])\n",
    "\n",
    "        if higher_better:\n",
    "            TP_temp =len(df_results_eval[(df_results_eval['actual_class'] == df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval[metric_name] >= val)])                            \n",
    "            FP_temp = len(df_results_eval[(df_results_eval['actual_class'] != df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval[metric_name] >= val)])\n",
    "            TN_temp = len(df_results_eval[(df_results_eval['actual_class'] != df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval[metric_name] < val)])\n",
    "            FN_temp = len(df_results_eval[(df_results_eval['actual_class'] == df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval[metric_name] < val)])          \n",
    "            #print('How many percent of the incorrect images we can miss to detect? = FAILURE RATE')\n",
    "#             fail_rate_temp = len(df_results_eval[(df_results_eval['Incorrect']==1) & \\\n",
    "#                     (df_results_eval[metric_name]>=val)])/len(df_results_eval)*100\n",
    "#             fail_rate.append(fail_rate_temp)\n",
    "            #print('How many percent of the images should be double-checked by the laborants? = WORK LOAD')\n",
    "#             work_load_temp = len(df_results_eval[(df_results_eval['Incorrect']==0) & \\\n",
    "#                     (df_results_eval[metric_name]<val)])/len(df_results_eval)*100\n",
    "            #work_load_temp = len(df_results_eval[df_results_eval[metric_name]<val])/len(df_results_eval)*100\n",
    "            \n",
    "#             len(tuc1.df_result_ensemble[(tuc1.df_result_ensemble['Incorrect']==1) & \\\n",
    "#                         (tuc1.df_result_ensemble['preds_max']>=0.78)].iloc[:,-11:])\n",
    "\n",
    "#             len(tuc1.df_result_ensemble[(tuc1.df_result_ensemble['Incorrect']==0) & \\\n",
    "#                                     (tuc1.df_result_ensemble['preds_max']<=0.78)].iloc[:,-11:])\n",
    "        else:\n",
    "            TP_temp =len(df_results_eval[(df_results_eval['actual_class'] == df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval[metric_name] < val)])                            \n",
    "            FP_temp = len(df_results_eval[(df_results_eval['actual_class'] != df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval[metric_name] < val)])\n",
    "            TN_temp = len(df_results_eval[(df_results_eval['actual_class'] != df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval[metric_name] >= val)])\n",
    "            FN_temp = len(df_results_eval[(df_results_eval['actual_class'] == df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval[metric_name] >= val)])\n",
    "            #print('How many percent of the incorrect images we can miss to detect? = FAILURE RATE'=FP)\n",
    "#             fail_rate_temp = len(df_results_eval[(df_results_eval['Incorrect']==1) & \\\n",
    "#                     (df_results_eval[metric_name]<val)])/len(df_results_eval)*100\n",
    "#             fail_rate.append(fail_rate_temp)\n",
    "            #print('How many percent of the images should be double-checked by the laborants? = WORK LOAD')\n",
    "#             work_load_temp = len(df_results_eval[(df_results_eval['Incorrect']==0) & \\\n",
    "#                     (df_results_eval[metric_name]>=val)])/len(df_results_eval)*100\n",
    "#             work_load_temp = len(df_results_eval[df_results_eval[metric_name]>=val])/len(df_results_eval)*100\n",
    "#             work_load.append(work_load_temp)\n",
    "\n",
    "        TP.append(TP_temp); FP.append(FP_temp); FN.append(FN_temp); TN.append(TN_temp)\n",
    "        fail_rate.append(FP_temp/len(df_results_eval)*100)\n",
    "        work_load_temp = (TN_temp + FN_temp)/len(df_results_eval)*100\n",
    "        work_load.append(work_load_temp)\n",
    "\n",
    "        TPR_temp = 0 if (TP_temp+FN_temp)==0 else TP_temp/(TP_temp+FN_temp)\n",
    "        TNR_temp = 0 if (TN_temp+FP_temp)==0 else TN_temp/(TN_temp+FP_temp)\n",
    "        FPR_temp = 0 if (FP_temp+TN_temp)==0 else FP_temp/(FP_temp+TN_temp)\n",
    "        # FDR (False Discovery Rate) = FP/(FP+TP)  ... FDR = 1 - precision=1-PPV\n",
    "        FDR_temp = 0 if (FP_temp+TP_temp)==0 else FP_temp/(FP_temp+TP_temp)\n",
    "        # NPV (Negative Predictive Value) = TN/(TN+FN)\n",
    "        NPV_temp = 0 if (TN_temp+FN_temp)==0 else TN_temp/(TN_temp+FN_temp)\n",
    "\n",
    "        # https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    "        x.append(val)\n",
    "        TPR.append(round(TPR_temp,4))  # TP/(TP+FN) = True Positive Rate (sensitivity, recall)\n",
    "        TNR.append(round(TNR_temp,4))  # TN/(TN+FP) = True Negative Rate (pecificity, selectivity)\n",
    "        FPR.append(round(FPR_temp,4))  # FP/(FP+TN) = False Positive Rate (fall-out)\n",
    "        FDR.append(round(FDR_temp,4))  # FP/(FP+TP) = False Discovery Rate \n",
    "        NPV.append(round(NPV_temp,4))  # TN/(TN+FN) = Negative Predictive Value \n",
    "                \n",
    "    df_Res = pd.DataFrame({'val': x, 'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN, 'fail_rate': fail_rate,\n",
    "                          'work_load': work_load})\n",
    "    df_Res['sum'] = df_Res['TN'] + df_Res['FN']\n",
    "    df_Res['diff'] = df_Res['work_load'] - df_Res['fail_rate']\n",
    "    \n",
    "    #arr_diff=np.array(TNR)-np.array(FPR)\n",
    "    # What is the threshold to capture the min. number of False Negative decisions\n",
    "    #arr_diff=np.array(NPV)-np.array(FDR)\n",
    "    arr_diff=np.array(work_load)-np.array(fail_rate)\n",
    "    # What is the threshold to capture the min. number of False Positive decisions\n",
    "#     arr_diff=np.array(FDR)-np.array(NPV)\n",
    "\n",
    "    # find the threshold giving the max diff between NPV (correct-NEGATIVE predictions) \n",
    "    #                                            and FDR (incorrect-POSITIVE predictions)\n",
    "    argmax = np.argmin(np.array(df_Res['diff']**2))\n",
    "    print(f'argmax:{argmax}')\n",
    "    best_threshold = round(x[argmax], 2)\n",
    "\n",
    "    lst_threshold = x[argmax-plus_minus:argmax+plus_minus]\n",
    "#     FDR_th = FDR[argmax-plus_minus:argmax+plus_minus]\n",
    "#     NPV_th = NPV[argmax-plus_minus:argmax+plus_minus]\n",
    "    lst_fail_rate = fail_rate[argmax-plus_minus:argmax+plus_minus]\n",
    "    lst_work_load = work_load[argmax-plus_minus:argmax+plus_minus]\n",
    "    # number of images expected to be incorrectly predicted after uncertainty/certainty filtering\n",
    "    n_total_mistakes = np.array(FP[argmax-plus_minus:argmax+plus_minus]) \\\n",
    "                     + np.array(FN[argmax-plus_minus:argmax+plus_minus])\n",
    "    rate_total_mistakes = np.array(n_total_mistakes)/len(df_results_eval)\n",
    "    rate_FP = np.array(FP[argmax-plus_minus:argmax+plus_minus])/len(df_results_eval)\n",
    "\n",
    "    df_threshold = pd.DataFrame({'threshold': lst_threshold, 'fail_rate': lst_fail_rate, \n",
    "                                 'work_load': lst_work_load, 'total_mistakes': n_total_mistakes, \n",
    "                                 'total_mistakes(%)': rate_total_mistakes, 'FP(%)': rate_FP})\n",
    "\n",
    "    print(f'\\nWhen threshold={best_threshold}:')\n",
    "    print(f'TP: {TP[argmax]} .. FP: {FP[argmax]} .. TN: {TN[argmax]} .. FN: {FN[argmax]}')\n",
    "    print(f'..... {FDR[argmax]}% of correctly classified images are below the threshold (FDR)')\n",
    "    print(f'while {NPV[argmax]}% of incorrectly classified images are below the threshold (NPV)')    \n",
    "    print(f'precision: {round(TP[argmax]/(TP[argmax]+FP[argmax]), 2)}')\n",
    "    print(f'recall: {round(TP[argmax]/(TP[argmax]+FN[argmax]), 2)}')\n",
    "\n",
    "    plt.plot(x, fail_rate)\n",
    "    plt.plot(x, work_load)\n",
    "    th_best = np.zeros(len(x))\n",
    "    th_best.fill(x[argmax])\n",
    "    th_best = list(th_best)\n",
    "    plt.plot(th_best, FDR)\n",
    "#     plt.plot(x, fail_rate)\n",
    "#     plt.plot(x, work_load)\n",
    "    \n",
    "    #plt.legend(['FDR', 'NPV', 'th_best', 'fail_rate', 'work_load'])\n",
    "    plt.legend(['fail_rate', 'work_load', 'th_best'])\n",
    "#     plt.xlabel('Threshold')\n",
    "#     plt.ylabel('% of Images')\n",
    "    plt.set_xlabel('Threshold')\n",
    "    plt.set_ylabel('% of Images')\n",
    "    #plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "#     plt.title(title)\n",
    "    plt.set_title(title)\n",
    "\n",
    "    # plt.annotate('best threshold', \n",
    "    #              xy=(lst_threshold[argmax], lst_unmatch[argmax]), \n",
    "    #              #xytext=(lst_threshold[argmax]-20, lst_unmatch[argmax]-0.2), \n",
    "    #              xytext=(20, 0.2), \n",
    "    #              arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "    #              )\n",
    "\n",
    "    plt.text(x[argmax]-0.2*x[argmax], .6, f'best threshold={round(x[argmax],2)}')\n",
    "    #plt.text(0.07, .92, f'best threshold={round(lst_threshold[argmax],2)}')\n",
    "    #plt.set_figure(figsize=(18, 18))\n",
    "    #plt.savefig(f'../graphs/best_score_{percent_of_images*100}_{round(y_match[i],4)*100}.png')\n",
    "#     plt.show()\n",
    "\n",
    "    print(df_threshold.to_string(index=False))        \n",
    "    \n",
    "    return df_Res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_results_eval[(df_results_eval['actual_class'] == df_results_eval['best_pred']) &\n",
    "                                         (df_results_eval['best_score'] >= .96)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Score - Softmax & Certainty (Top-2)\n",
    "#### (Horizontal - Between classes [after averaging the results for the ensembles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ((plt_sub1, plt_sub2)) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "# fig, ((plt_sub1)) = plt.subplots(nrows=1, ncols=1, figsize=(15,4))\n",
    "\n",
    "df_Softmax = plot_metrics_test(plt_sub1, title='Softmax Score', metric_name='best_score', step_size=0.01, is_norm=False)\n",
    "df_Top2 = plot_metrics_test(plt_sub2, title='Top-2 Difference', metric_name='Certainty', step_size=1, is_norm=False)\n",
    "\n",
    "plt.show()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(df_Softmax.index), list(df_Softmax['fail_rate']))\n",
    "plt.plot(list(df_Softmax.index), list(df_Softmax['work_load']))\n",
    "# th_best = np.zeros(len(x))\n",
    "# th_best.fill(x[argmax])\n",
    "# th_best = list(th_best)\n",
    "# plt.plot(th_best, FDR)\n",
    "\n",
    "plt.legend(['fail_rate', 'work_load'])\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('% of Images')\n",
    "# plt.set_xlabel('Threshold')\n",
    "# plt.set_ylabel('% of Images')\n",
    "#plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "#     plt.title(title)\n",
    "# plt.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "line1, = ax1.plot(list(df_Softmax['fail_rate']), color='blue', lw=2)\n",
    "line2, = ax2.plot(list(df_Softmax['work_load']), color='blue', lw=2)\n",
    "\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3.913709-3.622185)\n",
    "print((3.913709-3.622185)/3.622185*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3.913709-3.622185)/3.622185*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you accept to have 1% error rate, then you need to check 2.62% of the images (360) manually\n",
    "# (if I start searching the val from 0, work_load=2.59% ... if start the val from min_val, 2,62%)\n",
    "# threshold you will use is = 0.3287 for Softmax. You'll accept anything above this value as correctly classified\n",
    "df_Softmax[df_Softmax['fail_rate']<0.5] \n",
    "# (%) e.g. Fail Rate = FP/n_all_images\n",
    "# if 0.1 then you were guaranteed to have 99.9% success if you manually verify 773 images (5.63%) out of 13721 images\n",
    "# if   1 then you were guaranteed to have 99.0% success if you manually verify 356 images (2.59%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Softmax[df_Softmax['val']>=0.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_flexible = df_Softmax[(df_Softmax['fail_rate']>=0.5) & (df_Softmax['fail_rate']<=1)] \n",
    "df_flexible = df_Softmax[(df_Softmax['work_load']<=5) & (df_Softmax['fail_rate']<=1)] \n",
    "lst_flexible = list(df_flexible['val'])\n",
    "lst_flexible_workload = list(df_flexible['work_load'])\n",
    "print(f'{len(lst_flexible)} values from {min(lst_flexible)} to {max(lst_flexible)} having fail rate between 0.5 and 1.0')\n",
    "print(f'workload from {min(lst_flexible_workload)} to {max(lst_flexible_workload)}')\n",
    "print(3.913709-3.622185)\n",
    "print((3.913709-3.622185)/3.622185*100)\n",
    "\n",
    "df_flexible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_Softmax['effectiveness'] = (df_Softmax['fail_rate']*.66) + (df_Softmax['work_load']*.33)\n",
    "df_Softmax['effectiveness2'] = (df_Softmax['fail_rate']*.75) + (df_Softmax['work_load']*.25)\n",
    "df_Softmax['effectiveness3'] = (df_Softmax['fail_rate']*.7) + (df_Softmax['work_load']*.3)\n",
    "df_Softmax\n",
    "#df_Softmax[(df_Softmax['effectiveness']>1.55) & (df_Softmax['effectiveness']<1.65)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Top2[df_Top2['fail_rate']<0.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flexible = df_Top2[(df_Top2['work_load']<=5) & (df_Top2['fail_rate']<=1)] \n",
    "lst_flexible = list(df_flexible['val'])\n",
    "lst_flexible_workload = list(df_flexible['work_load'])\n",
    "print(f'{len(lst_flexible)} values from {min(lst_flexible)} to {max(lst_flexible)} having fail rate between 0.5 and 1.0')\n",
    "print(f'workload from {min(lst_flexible_workload)} to {max(lst_flexible_workload)}')\n",
    "\n",
    "print(3.913709-3.622185)\n",
    "print((3.913709-3.622185)/3.622185*100)\n",
    "\n",
    "df_flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_results_eval[df_results_eval['best_score']<=0.3287])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_results_eval[(df_results_eval['best_score']<=0.387) & df_results_eval['Incorrect']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_results_eval[df_results_eval['best_score']<=0.3487])/len(df_results_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_results_eval)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1 = [1,2,3,4]\n",
    "lst_2 = ['a', 'b', 'c', 'd']\n",
    "dict_x = {'lst_1': lst_1, 'lst_2': lst_2}\n",
    "dict_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy of Ensemble-Softmax [sum of entr(means)] & VAR\n",
    "#### (Horizontal - Between classes [after averaging the results for the ensembles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((plt_sub1, plt_sub2)) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "'''\n",
    "here, we calculate the entropy of the mean of the predictions e.g.:\n",
    "sm_e.i = Softmax value of the 'e'th ensemble of 'i'th class\n",
    "ensemble_model1 = [sm_1.1, sm_1.2, ..., sm_1.24]\n",
    "ensemble_model2 = [sm_2.1, sm_2.2, ..., sm_2.24]\n",
    "sm1 = (sm_1.1 + sm_2.1)/2\n",
    "ensemble_model_mean = [sm1, sm2, ..., sm24]\n",
    "entropy will be calculated for ensemble_model_mean and sum of those individual means\n",
    "HIGH entropy indicates HIGH information, which means HIGH UNCERTAINTY. so, we take the reverse of it \n",
    "to measure the certainty. THEN HIGH CERTAINTY is expected for CORRECTLY classified images.\n",
    "LOW entropy indicates LOW information, which means LOW UNCERTAINTY=HIGH CERTAINTY.\n",
    "NOTE: This entropy values normalized according to the highest possible entropy, so the max they can take is 1.0.\n",
    "But since it is almost impossible have E_max, we see the max<1\n",
    "'''\n",
    "\n",
    "df_Entropy_H = plot_metrics_test(plt_sub1, title='Entropy of Ensemble-Softmax [sum of entr(means)]', metric_name='u_entr',\n",
    "             step_size=0.0096, higher_better=False, is_norm=False)\n",
    "\n",
    "'''\n",
    "var will be calculated for ensemble_model_mean\n",
    "HIGH variance indicates that the values are spreaded out from each other.\n",
    "LOW variance indicates that the data points tend to be very close to the mean.\n",
    "HIGH variance indicates that the data points are very spread out from the mean, and from one another.\n",
    "so, we expect HIGHer variance value if there is a class which is much higher than others.\n",
    "'''\n",
    "df_Var_H = plot_metrics_test(plt_sub2, title='var', metric_name='var', step_size=0.0004, higher_better=True, is_norm=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_Entropy_H[df_Entropy_H['fail_rate']>=0.45]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flexible = df_Entropy_H[(df_Entropy_H['work_load']<=5) & (df_Entropy_H['fail_rate']<=1)] \n",
    "lst_flexible = list(df_flexible['val'])\n",
    "lst_flexible_workload = list(df_flexible['work_load'])\n",
    "print(f'{len(lst_flexible)} values from {min(lst_flexible)} to {max(lst_flexible)} having fail rate between 0.5 and 1.0')\n",
    "print(f'workload from {min(lst_flexible_workload)} to {max(lst_flexible_workload)}')\n",
    "\n",
    "print(3.731506-3.622185)\n",
    "print((3.731506-3.622185)/3.622185*100)\n",
    "\n",
    "df_flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Var_H[df_Var_H['fail_rate']<0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flexible = df_Var_H[(df_Var_H['work_load']<=5) & (df_Var_H['fail_rate']<=1)] \n",
    "lst_flexible = list(df_flexible['val'])\n",
    "lst_flexible_workload = list(df_flexible['work_load'])\n",
    "print(f'{len(lst_flexible)} values from {min(lst_flexible)} to {max(lst_flexible)} having fail rate between 0.5 and 1.0')\n",
    "print(f'workload from {min(lst_flexible_workload)} to {max(lst_flexible_workload)}')\n",
    "\n",
    "print(3.753371-3.622185)\n",
    "print((3.753371-3.622185)/3.622185*100)\n",
    "\n",
    "df_flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bhattacharyya Coefficient & JSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((plt_sub1, plt_sub2)) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "\"\"\"\n",
    "Bhattacharyya Coefficient (BC):\n",
    "The Bhattacharyya coefficient is a measure of the amount of overlap between \n",
    "two statistical samples or populations.\n",
    "\"\"\"\n",
    "df_results_eval['BC']=df_results_eval['best_score']*df_results_eval['best_2nd_score']\n",
    "df_BC=plot_metrics_test(plt_sub1, title='Bhattacharyya Coefficient', metric_name='BC', step_size=0.01, is_norm=False)\n",
    "df_JSD=plot_metrics_test(plt_sub2, title='JSD', metric_name='P_jsd', step_size=0.01, higher_better=True, is_norm=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_JSD[df_JSD['fail_rate']>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BC[df_BC['fail_rate']>0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## var_Top1 (Vertical - Between ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((plt_sub1)) = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\n",
    "lst_var = []\n",
    "'''\n",
    "here, we calculate the variance of the best predictions among the ensemble members. e.g.:\n",
    "sm_e.i = Softmax value of the 'e'th ensemble of 'i'th class\n",
    "ensemble_model1 = [sm_1.1, sm_1.2, ..., sm_1.24]\n",
    "ensemble_model2 = [sm_2.1, sm_2.2, ..., sm_2.24]\n",
    "if the highest mean belongs to class_2, then\n",
    "var will be calculated for sm_1.2 and sm_2.2\n",
    "The variance in this case shows how much the models agree on a decision unlike the variance calculated\n",
    "for the entire class results of mean-ensemble values.\n",
    "'''\n",
    "\n",
    "for x in df_results_eval.index:\n",
    "    best_index = df_results_eval.loc[x, 'best_index']\n",
    "    lst_var.append(np.var(tuc1.df_result_ensemble.loc[x, 'outputs_all'][:, best_index]))\n",
    "\n",
    "df_results_eval['var_Top1'] = lst_var\n",
    "# plot_metrics(title='var_Top1', metric_name='var_Top1', min_max_step=(0.9, 0.99, 0.01), is_norm=False)\n",
    "df_var_Top1=plot_metrics_test(plt_sub1, title='var_Top1', metric_name='var_Top1', step_size=0.001, higher_better=True, is_norm=False)\n",
    "df_Var_H = plot_metrics_test(plt_sub2, title='var', metric_name='var', step_size=0.0005, higher_better=True, is_norm=False)\n",
    "\n",
    "#plot_metrics_p(plt_sub3, title='Softmax Score', metric_name='best_score', step_size=0.01, is_norm=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[1, 'outputs_all']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy (same as above (horizontal-entropy) but this time not normalized according to the max value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((plt_sub1)) = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\n",
    "plot_metrics(plt_sub1, title='Entr', metric_name='entr', step_size=0.01, higher_better=False, is_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STD (Horizontal - Between classes [after averaging the results for the ensembles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_metrics(title='std', metric_name='std', min_max_step=(0.9, 0.99, 0.01), is_norm=False)\n",
    "fig, ((plt_sub1)) = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\n",
    "plot_metrics(plt_sub1, title='std', metric_name='std', step_size=0.001, higher_better=True, is_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuc1.df_result_ensemble[:1]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "c_list = ['picture','uncertainty', 'actual_class', 'best_pred','best_score', 'best_2nd_score', 'P_jsd', 'P_info', 'std', 'u_entr', 'var_Top1']\n",
    "df_results_eval[df_results_eval['actual_class'] != df_results_eval['best_pred']][c_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval[df_results_eval['actual_class'] == df_results_eval['best_pred']][c_list][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble[tuc1.df_result_ensemble['picture'] == 'MA160814197500041.070-K_4_0_.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# xx=np.array([1,2,3,4,0])\n",
    "xx=np.array([1/5,1/5,1/5,1/5,1/5])\n",
    "Counter(xx)\n",
    "# print(np.bincount(xx))\n",
    "# np.var(np.bincount(xx))\n",
    "# entr(np.bincount(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble[tuc1.df_result_ensemble['picture'] == 'MA160873676300027.057-K_20_1_.png']['outputs_all'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.show_results(result = 'ALL') # mean=mean top-2 uncertainty of the given class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=np.array([1,2,8,4,5])\n",
    "np.argmax(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.show_results(result = 'match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.show_results(result = 'unmatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.show_results(result = 'combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "tuc1.show_results(result = 'ALL2nd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval[df_results_eval['actual_class'] != \n",
    "                df_results_eval['best_pred']].loc[:,['picture','actual_class', 'best_pred', 'Certainty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.loc[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_uncertainty.loc[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval[df_results_eval['actual_class'] == \n",
    "                df_results_eval['best_pred']].loc[:,['picture','actual_class', 'best_pred', 'Certainty']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "plt.figure()\n",
    "class_names = range(24)\n",
    "plot_confusion_matrix(confusion, classes=class_names, normalize=False,title='Normalized confusion matrix')\n",
    "# plot_confusion_matrix(confusion, normalize=True,title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "l_label = list(df_results_eval['actual_class'])\n",
    "l_pred = list(df_results_eval['best_pred'])\n",
    "performance.plot_confusion_matrix(l_label, l_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(l_label, l_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/multi-class-classification-extracting-performance-metrics-from-the-confusion-matrix-b379b427a872\n",
    "#https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826#:~:text=False%20Positive%20(FP)%3A%20It,the%20positive%20class%20as%20negative.\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum_TP)/(sum_TP+sum_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_TP = sum(TP)\n",
    "sum_TN = sum(TN)\n",
    "sum_FN = sum(FN)\n",
    "sum_FP = sum(FP)\n",
    "sum_FN/(sum_TP+sum_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum_TP+sum_TN)/(sum_TP+sum_FP+sum_FN+sum_TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_FN/(sum_TP+sum_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_label_idx = list(df_results_eval['class_idx'])\n",
    "l_pred_idx = list(df_results_eval['best_index'])\n",
    "l_predict_proba = list(df_results_eval['Mean'])\n",
    "roc_auc_score(l_label_idx, l_predict_proba, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "arr_label_idx = np.array(l_label_idx).reshape(-1,1)\n",
    "arr_predict_proba = np.array(l_predict_proba)\n",
    "\n",
    "#label_encoder = LabelEncoder()\n",
    "#l_label_int = label_encoder.fit_transform(l_label_int)    \n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(arr_label_idx)   # onehot_encoded => array(852,24)=(n_samples, n_classes)\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"ALL\"], tpr[\"ALL\"], _ = roc_curve(onehot_encoded.ravel(), arr_predict_proba.ravel())\n",
    "roc_auc[\"ALL\"] = auc(fpr[\"ALL\"], tpr[\"ALL\"])\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[\"ALL\"], tpr[\"ALL\"], color='darkorange',\n",
    "lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[\"ALL\"])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "fpr_ALL, tpr_ALL, roc_auc_val = fpr['ALL'], tpr['ALL'], roc_auc[\"ALL\"]\n",
    "# print(f\"fpr_ALL: {fpr_ALL} .. ttpr_ALL: {tpr_ALL} .. roc_auc_val: {roc_auc_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arr_label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics.confusion_matrix(l_label, l_pred))\n",
    "print(metrics.classification_report(l_label, l_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_label_idx = np.array(l_label_idx).reshape(-1,1)\n",
    "arr_predict_proba = np.array(l_predict_proba)\n",
    "\n",
    "# plt_class = for which class you want to plot the ROC CURVE\n",
    "fpr_micro, tpr_micro, roc_auc_val = \\\n",
    "performance.plot_roc_curve(arr_label_idx, arr_predict_proba, n_classes=24, plt_class=22)\n",
    "\n",
    "print(round(roc_auc_val,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_results_eval[df_results_eval['picture']=='MA160814197500042.017-K_9_0_.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_eval\n",
    "# output = tuc1.softmax_and_reshape(tuc1.df_result_ensemble.loc[0,'outputs_all'])\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import entr\n",
    "# entr(output[:,8]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# entr_per_model=entr(output).sum(axis=1)/np.log(2)\n",
    "# entr_per_model2=entr(output).sum(axis=0)/np.log(2)\n",
    "# print(entr_per_model)\n",
    "# print(np.mean(entr_per_model))\n",
    "# print(entr_per_model2)\n",
    "# print(np.mean(entr_per_model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=0\n",
    "# # variance of the scores of the best prediction (pred with the highest score)\n",
    "# # indexes are preserved between 2 dataframes\n",
    "# np.var(tuc1.normalize_array_preprocess(tuc1.df_result_ensemble.loc[x,\n",
    "#                                         'outputs_all'])[:,df_results_eval.loc[x,'best_index']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_results_eval)):\n",
    "    if i %1000 == 0:\n",
    "        pic1 = df_results_eval.loc[i,'picture']\n",
    "        pic2 = tuc1.df_result_ensemble.loc[i,'picture']\n",
    "        print(f'df_results_eval: {pic1} ... df_result_ensemble: {pic2}')\n",
    "    if df_results_eval.loc[i,'picture'] != tuc1.df_result_ensemble.loc[i,'picture']:\n",
    "        print('mismatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[0,'outputs_all'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[:0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble['outputs_all'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "t_test=tuc1.softmax_and_reshape(tuc1.df_result_ensemble.loc[x, 'outputs_all'])[:,df_results_eval.loc[x,'best_index']]\n",
    "t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tuc1.df_result_ensemble.loc[0, 'outputs_all'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[0, 'outputs_all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[0, 'outputs_all'][:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "softmax(tuc1.df_result_ensemble.loc[0, 'outputs_all'][:,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "tuc1.normalize_array_preprocess(tuc1.df_result_ensemble.loc[x,'outputs_all'])[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.normalize_array_preprocess(tuc1.df_result_ensemble.loc[x,'outputs_all'])[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "np.argmax(tuc1.normalize_array_preprocess(tuc1.df_result_ensemble.loc[x,'outputs_all'])[1:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "np.argmax(tuc1.df_result_ensemble.loc[:,'outputs_all'][x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[:10,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
