{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.7.1\n",
      "Torchvision Version:  0.8.2\n"
     ]
    }
   ],
   "source": [
    "import datetime                  # test + Uncertainty\n",
    "import pandas as pd              # test + Uncertainty\n",
    "import numpy as np               # test + Uncertainty\n",
    "import pickle5 as pickle                    # Uncertainty\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import test_model as tm          # test\n",
    "import test_uncertainty_Softmax_v4 as tuc   # Uncertainty\n",
    "from humanfriendly import format_timespan\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import performance\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.special import entr\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN TEST ON PICTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty running: 2021-07-05 13:55:30.535612\n"
     ]
    }
   ],
   "source": [
    "# execute_test = False\n",
    "# start_date=datetime.datetime.now()\n",
    "# if execute_test:        \n",
    "#     %run test_model --config ../config/config_squeezenet1_0.ini\n",
    "    \n",
    "pd.set_option('max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', -1)\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "start_date=datetime.datetime.now()\n",
    "print(f'uncertainty running: {start_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN UNCERTAINTY CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the DataFrame of the ensemble results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-05 13:55:47,487  uncertainty   INFO  initializing uncertainty calculations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picking the models to be used\n",
      "date and time = 05.07.2021_13.55.47\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "# tuc1 = tuc.test_uncertainty('../config/config_squeezenet1_0.ini')\n",
    "# tuc1 = tuc.test_uncertainty('../config/config_alexnet.ini')\n",
    "# tuc1 = tuc.test_uncertainty('../config/config_resnet18.ini')\n",
    "# use_sample_weights = [0,1,3,5,6]\n",
    "use_sample_weights = [-1]  # if -1 then use all weights in the given ***** result pkl ******\n",
    "tuc1 = tuc.test_uncertainty('../config/TEST/config_densenet121_TTA.ini', use_sample_weights)\n",
    "#tuc1 = tuc.test_uncertainty('../config/config_resnet18.ini', use_sample_weights)\n",
    "#tuc1 = tuc.test_uncertainty('../config/TEST/config_densenet121_SCRATCH.ini', use_sample_weights)\n",
    "#tuc1 = tuc.test_uncertainty('../config/TEST/config_densenet121_DROPOUT.ini', use_sample_weights)\n",
    "u_Dataset = 'arkusai'\n",
    "u_Model = 'alexnet'\n",
    "u_Data = 'VAL'\n",
    "#tuc1 = tuc.test_uncertainty('../config/VAL/config_resnet18_SCRATCH.ini', use_sample_weights)\n",
    "print(list(range(len(tuc1.df_result_ensemble.loc[0,'outputs_all']))))\n",
    "\n",
    "index_to_class = pd.read_csv('../data/index_to_class.csv')\n",
    "class_to_index = index_to_class.set_index('class')\n",
    "class_to_index.columns=['idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96.2102, 96.1893, 96.1788, 96.1893, 96.0951, 96.1788, 96.3987, 96.2835, 96.2312, 96.0532, 96.2416, 96.116]\n"
     ]
    }
   ],
   "source": [
    "lst_acc_models, lst_column = tuc1.accuracy_per_model(use_sample_weights)\n",
    "print(lst_acc_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-05 13:55:49,582  uncertainty   INFO  Evaluating top-2 related uncertainty\n",
      "/Users/melih/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/___Master Thesis/PyTorch_Exercise/arkusai/code/test_uncertainty_Softmax_v4.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_result_ensemble['outputs_all'][i] = output\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cffc2602f464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0macc_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#acc_res = (accuracy, c_unmatch, c_2nd_match, lst_worst_acc)  # c=count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdf_results_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_uncertainty\u001b[0m  \u001b[0;31m# df_final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/___Master Thesis/PyTorch_Exercise/arkusai/code/test_uncertainty_Softmax_v4.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mdf_result_ensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_result_ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_top2_Softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_picture\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/___Master Thesis/PyTorch_Exercise/arkusai/code/test_uncertainty_Softmax_v4.py\u001b[0m in \u001b[0;36meval_top2_Softmax\u001b[0;34m(self, n_class)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m#output = self.normalize_array_preprocess(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_and_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mdf_result_ensemble\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputs_all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;31m#             df_result_ensemble.loc[i, 'outputs_all'] = output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_with_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_update_cacher\u001b[0;34m(self, clear, verify_is_copy)\u001b[0m\n\u001b[1;32m   3346\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3347\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3348\u001b[0;31m                     \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cache_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcacher\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3349\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3350\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_cache_changed\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   3303\u001b[0m         \"\"\"The object has called back to us saying maybe it has changed.\n\u001b[1;32m   3304\u001b[0m         \"\"\"\n\u001b[0;32m-> 3305\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mblk_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m                 \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0munfit_mgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, locs, values)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Resnet & Densenet\n",
    "#WL_max = 5; FR_min=0.5; FR_max=1.0\n",
    "\n",
    "# Squeezenet\n",
    "WL_max = 10; FR_min=0.5; FR_max=1.0\n",
    "# WL_max = 6; FR_min=0.5; FR_max=1.0\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "acc_res = tuc1.run()\n",
    "#acc_res = (accuracy, c_unmatch, c_2nd_match, lst_worst_acc)  # c=count\n",
    "df_results_eval = tuc1.df_uncertainty  # df_final\n",
    "df_final_summary = tuc1.df_final_summary\n",
    "\n",
    "l_column = tuc1.enhance_evaluations(lst_column)\n",
    "\n",
    "df_results_eval['v_entropy'] = tuc1.df_result_ensemble['v_entropy']\n",
    "df_results_eval['preds_rate'] = tuc1.df_result_ensemble['preds_rate']\n",
    "\n",
    "tuc1.min_max_norm(df_results_eval, metric='var')\n",
    "df_results_eval.rename(columns={'u_entr': 'u_entr1'}, inplace=True)\n",
    "tuc1.min_max_norm(df_results_eval, metric='entr')\n",
    "tuc1.min_max_norm(df_results_eval, metric='var')\n",
    "\n",
    "df_ensemble_res, df_flexible = tuc1.vertical_check(lst_column, upper_th=1, lower_th=0.5)\n",
    "\n",
    "#model_similarities(n_models)\n",
    "\n",
    "#tuc1.df_combine\n",
    "\n",
    "# fig, ((plt_sub1, plt_sub2)) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "df_Softmax = tuc1.plot_metrics_test(plt_m=None, title='Softmax Score', metric_name='best_score', \n",
    "                                    step_size=0.01, is_norm=False, plus_minus = 5, plot_graph = False)\n",
    "df_Top2 = tuc1.plot_metrics_test(plt_m=None, title='Top-2 Difference', metric_name='Certainty', step_size=1, \n",
    "                            is_norm=False, plus_minus = 5, plot_graph = False)\n",
    "df_uncertainty = tuc1.plot_metrics_test(plt_m=None, title='Top-2_Uncertainty', metric_name='uncertainty', \n",
    "                        step_size=0.01, higher_better=False, is_norm=False, plus_minus = 5, plot_graph = False)\n",
    "\n",
    "#df_Softmax[df_Softmax['fail_rate']<0.5] # take the first row\n",
    "#df_Top2[df_Top2['fail_rate']<0.5] \n",
    "\n",
    "df_range_Softmax = tuc1.tolerance_range(df_Softmax, FR_min=FR_min, FR_max=FR_max, WL_max = WL_max)\n",
    "###### df_range_Softmax                      \n",
    "\n",
    "df_range_Top2 = tuc1.tolerance_range(df_Top2, FR_min=FR_min, FR_max=FR_max, WL_max = WL_max)\n",
    "\n",
    "df_range_uncertainty = tuc1.tolerance_range(df_uncertainty, FR_min=FR_min, FR_max=FR_max, WL_max = WL_max)\n",
    "###### df_range_Top2\n",
    "\n",
    "######  ************************** #######\n",
    "\n",
    "# df_Entropy_H = tuc1.plot_metrics_test(plt_m=None, title='Entropy of Ensemble-Softmax [sum of entr(means)]', \n",
    "#            metric_name='u_entr', step_size=0.0096, higher_better=False, is_norm=False, plus_minus = 5, \n",
    "#                                       plot_graph = False)\n",
    "\n",
    "# df_Var_H = tuc1.plot_metrics_test(plt_m=None, title='var', metric_name='var', step_size=0.0004, \n",
    "#                        higher_better=True, is_norm=False, plus_minus = 5, plot_graph = False)\n",
    "\n",
    "df_Entropy_H = tuc1.plot_metrics_test(plt_m=None, title='Entropy of Ensemble-Softmax [sum of entr(means)]', \n",
    "           metric_name='u_entr', step_size=0.01, higher_better=False, is_norm=False, plus_minus = 5, \n",
    "                                      plot_graph = False)\n",
    "\n",
    "df_Var_H = tuc1.plot_metrics_test(plt_m=None, title='u_var', metric_name='u_var', step_size=0.01, \n",
    "                       higher_better=True, is_norm=False, plus_minus = 5, plot_graph = False)\n",
    "\n",
    "###### df_Entropy_H[df_Entropy_H['fail_rate']>=0.45]\n",
    "df_range_Entropy_H = tuc1.tolerance_range(df_Entropy_H, FR_min=FR_min, FR_max=FR_max, WL_max = WL_max)\n",
    "###### df_range_Entropy_H\n",
    "\n",
    "######df_Var_H[df_Var_H['fail_rate']<0.5]\n",
    "df_range_Var_H = tuc1.tolerance_range(df_Var_H, FR_min=FR_min, FR_max=FR_max, WL_max = WL_max)\n",
    "###### df_range_Var_H\n",
    "\n",
    "model = df_results_eval.loc[0, 'model']\n",
    "#parse_metrics(model, acc_res, df_metric, df_range_metric, KPI='fail_rate', th_KPI = 0.5, is_smaller = True)\n",
    "df_Report_Softmax = tuc1.parse_metrics(model, acc_res, df_Softmax, df_range_Softmax)\n",
    "df_Report_Top2 = tuc1.parse_metrics(model, acc_res, df_Top2, df_range_Top2)\n",
    "df_Report_Uncertainty = tuc1.parse_metrics(model, acc_res, df_uncertainty, df_range_uncertainty)\n",
    "df_Report_Entr = tuc1.parse_metrics(model, acc_res, df_Entropy_H, df_range_Entropy_H)\n",
    "df_Report_Var = tuc1.parse_metrics(model, acc_res, df_Var_H, df_range_Var_H)\n",
    "\n",
    "dict_uncertainty = tuc1.find_mean(df_results_eval, metric = 'uncertainty')     # [0:100] lower better ... uncertainty: [0: 1]\n",
    "dict_Certainty = tuc1.find_mean(df_results_eval, metric = 'Certainty')     # [0:100] higher better ... uncertainty: [0: 1]\n",
    "dict_best_score = tuc1.find_mean(df_results_eval, metric = 'best_score')    # [0:100] higher better = Softmax\n",
    "dict_u_entr = tuc1.find_mean(df_results_eval, metric = 'u_entr')        # [0:1]   lower better  = normalized Entropy: e/e_max\n",
    "dict_u_var = tuc1.find_mean(df_results_eval, metric = 'u_var')         # [0:1]   higher better = normalized_min_max\n",
    "#tuc1.find_mean(df_results_eval, metric = 'var')          # [?:?]   higher better = NOT normalized\n",
    "#df_Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Softmax[(df_Softmax['work_load']<6) & (df_Softmax['fail_rate']<1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_Certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_u_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_u_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metrics = {'Softmax': dict_best_score, 'Certainty': dict_Certainty, 'uncertainty': dict_uncertainty\n",
    "               , 'Entropy': dict_u_entr, 'Variance': dict_u_var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metrics['Softmax']['mean_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI = 'fail_rate'\n",
    "# th_KPI = 0.5\n",
    "# df_Softmax[df_Softmax[KPI] < th_KPI].idxmin()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Report = tuc1.parse_metrics(model, acc_res, df_Softmax, df_range_Softmax)\n",
    "df_Report_Softmax = tuc1.parse_metrics(model, acc_res, df_Softmax, df_range_Softmax)\n",
    "df_Report_Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Report_Top2 = tuc1.parse_metrics(model, acc_res, df_Top2, df_range_Top2)\n",
    "df_Report_Top2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Report_uncertainty = tuc1.parse_metrics(model, acc_res, df_uncertainty, df_range_uncertainty, KPI='fail_rate', \n",
    "                                    th_KPI = 0.5, is_smaller = False)\n",
    "df_Report_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Report_Entr = tuc1.parse_metrics(model, acc_res, df_Entropy_H, df_range_Entropy_H, KPI='fail_rate', \n",
    "                                    th_KPI = 0.5, is_smaller = False)\n",
    "df_Report_Entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Report_Var = tuc1.parse_metrics(model, acc_res, df_Var_H, df_range_Var_H)\n",
    "df_Report_Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_reports = {'Softmax': df_Report_Softmax, 'Certainty': df_Report_Top2, 'uncertainty': df_Report_uncertainty\n",
    "               , 'Entropy': df_Report_Entr, 'Variance': df_Report_Var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names =  ['DataSet', 'Model', 'Data', 'Accuracy', 'Metric', 'Fail_Rate', 'Work_Load_P', 'Value'\n",
    "              , 'Tolerance_Range', 'Mean_ALL', 'Mean_Correct', 'Mean_Incorrect', 'Acc_Per_Model'\n",
    "              , 'Worst_Acc_Per_Class']\n",
    "df_Summary  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "# u_Dataset, u_Model, u_Data\n",
    "# Softmax, Certainty, uncertainty, Entropy, Variance\n",
    "for i, metric in enumerate(dict_metrics.keys()):\n",
    "    temp_row = [u_Dataset, u_Model, u_Data, acc_res, metric, dict_reports[metric]['fail_rate_m'][0]\n",
    "               , dict_reports[metric]['work_load_m'][0], dict_reports[metric]['val_m'][0]\n",
    "               ,f\"{dict_reports[metric]['range_len'][0]} ({dict_reports[metric]['range_min'][0]}-\\\n",
    "{dict_reports[metric]['range_max'][0]})\"\n",
    ", dict_metrics[metric]['mean_all'], dict_metrics[metric]['mean_correct'], dict_metrics[metric]['mean_incorrect']\n",
    "        , lst_acc_models,  dict_reports[metric]['worst_acc'][0]]\n",
    "    df_Summary.loc[i] = temp_row\n",
    "\n",
    "df_Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f'uncertainty_{u_Dataset}_{u_Model}_{u_Data}'\n",
    "df_Summary.to_csv(f'{save_name}.csv')\n",
    "save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_save_all = {'tuc1': tuc1\n",
    "# with open('config.dictionary', 'wb') as dictionary:\n",
    "#     pickle.dump(config_dictionary, config_dictionary_file)\n",
    "\n",
    "a_file = open(f'{save_name}.pkl', 'wb')\n",
    "pickle. dump(tuc1, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ((plt_sub1, plt_sub2)) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "\n",
    "# df_Softmax = tuc1.plot_metrics_test(plt_m=plt_sub1, title='Softmax Score', metric_name='best_score', \n",
    "#                                     step_size=0.01, is_norm=False, plus_minus = 5, plot_graph = True)\n",
    "\n",
    "# df_Top2 = tuc1.plot_metrics_test(plt_m=plt_sub2, title='Top-2 Difference', metric_name='Certainty', step_size=1, \n",
    "#                             is_norm=False, plus_minus = 5, plot_graph = True)\n",
    "\n",
    "# plt.savefig('../figures/test.png')\n",
    "# plt.show()\n",
    "# # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close('all')\n",
    "fig, ((plt_sub1, plt_sub2), (plt_sub3, plt_sub4)) = plt.subplots(nrows=2, ncols=2, figsize=(15,8))\n",
    "\n",
    "df_Softmax = tuc1.plot_metrics_test(plt_m=plt_sub1, title='Softmax Score', metric_name='best_score', \n",
    "                                    step_size=0.01, is_norm=False, plus_minus = 5, plot_graph = True)\n",
    "\n",
    "df_Top2 = tuc1.plot_metrics_test(plt_m=plt_sub2, title='Top-2 Difference', metric_name='Certainty', step_size=1, \n",
    "                            is_norm=False, plus_minus = 5, plot_graph = True)\n",
    "\n",
    "df_Entropy_H = tuc1.plot_metrics_test(plt_m=plt_sub3, title='u_entr', metric_name='u_entr', \n",
    "                step_size=0.01, higher_better=False, is_norm=False, plus_minus = 5, plot_graph = True)\n",
    "\n",
    "df_Var_H = tuc1.plot_metrics_test(plt_m=plt_sub4, title='u_var', metric_name='u_var', step_size=0.01, \n",
    "                            is_norm=False, plus_minus = 5, plot_graph = True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/test3.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ((plt_sub1, plt_sub2)) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "\n",
    "# df_Entropy_H = tuc1.plot_metrics_test(plt_m=plt_sub1, title='u_entr', metric_name='u_entr', \n",
    "#                 step_size=0.01, higher_better=False, is_norm=False, plus_minus = 5, plot_graph = True)\n",
    "\n",
    "# df_Var_H = tuc1.plot_metrics_test(plt_m=plt_sub2, title='u_var', metric_name='u_var', step_size=0.01, \n",
    "#                             is_norm=False, plus_minus = 5, plot_graph = True)\n",
    "\n",
    "# plt.savefig('../figures/test2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval_test=df_results_eval.loc[:,['Certainty','uncertainty', 'Incorrect', \n",
    "                                            'best_score', 'u_entr', 'u_var']]\n",
    "df_results_eval_test = df_results_eval_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_OLD(data_1, data_2, style='darkgrid', kde=True, bins=10):\n",
    "    sns.set_style(style)\n",
    "    sns.distplot(data_1, kde=kde, bins=bins)\n",
    "    sns_plot = sns.distplot(data_2, kde=kde, bins=bins)\n",
    "    \n",
    "    return sns_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric='uncertainty'\n",
    "metric='u_var'\n",
    "d_match = df_results_eval[df_results_eval['actual_class'] == df_results_eval['best_pred']][metric]\n",
    "d_unmatch = df_results_eval[df_results_eval['actual_class'] != df_results_eval['best_pred']][metric]\n",
    "# histogram_OLD(d_match, d_unmatch)\n",
    "sns_plot = histogram_OLD(d_match, d_unmatch)\n",
    "# sns_plot.figure.savefig(\"output.png\")\n",
    "#sns_plot.figure.savefig(\"../figures/output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric='u_var'\n",
    "d_match = df_results_eval[df_results_eval['actual_class'] == df_results_eval['best_pred']][metric]\n",
    "d_unmatch = df_results_eval[df_results_eval['actual_class'] != df_results_eval['best_pred']][metric]\n",
    "histogram_OLD(d_match, d_unmatch)\n",
    "\n",
    "# NEW one (but the old one is better)\n",
    "# sns.displot(df_results_eval_test, x='uncertainty', hue='Incorrect', kind='kde',\n",
    "#           common_norm=False, aspect=2, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_results_eval_test, x='uncertainty', hue='Incorrect', stat='probability',\n",
    "           common_norm=False, aspect=2, height=5, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(\n",
    "    data=df_results_eval_test[df_results_eval_test['Incorrect']==1], x=\"uncertainty\", y=\"best_score\",\n",
    "    marker=\"+\", s=50, marginal_kws=dict(bins=10, fill=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(\n",
    "    data=df_results_eval_test[df_results_eval_test['Incorrect']==0], x=\"uncertainty\", y=\"best_score\",\n",
    "    marker=\"+\", s=50, marginal_kws=dict(bins=10, fill=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes time=30 secs\n",
    "sns.jointplot(data=df_results_eval_test, x='uncertainty', y='best_score', hue='Incorrect',\n",
    "             kind='kde', common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df_results_eval_test, x='uncertainty', y='best_score', hue='Incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Incorrect', y='uncertainty', kind='boxen',\n",
    "           data=df_results_eval_test.sort_values('Incorrect'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes time=90 secs\n",
    "sns.catplot(x='uncertainty', y='best_score', row='Incorrect', kind='box',\n",
    "            orient='v', height=4.5, aspect=4,\n",
    "           data=df_results_eval_test.query('best_score>0.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes time=90 secs\n",
    "sns.catplot(x='best_score', y='u_var', row='Incorrect', kind='box',\n",
    "            orient='v', height=4.5, aspect=4,\n",
    "           data=df_results_eval_test)\n",
    "\n",
    "#data=df_results_eval_test.query('best_score>0.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RUN_UNTIL_HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes too much time and not that good visualisation\n",
    "g=sns.JointGrid()\n",
    "x, y = df_results_eval_test[df_results_eval_test['Incorrect']==1]['uncertainty'], df_results_eval_test['best_score']\n",
    "sns.scatterplot(x=x,y=y,ec='b',fc='none',s=100,linewidth=1.5,ax=g.ax_joint)\n",
    "sns.histplot(x=x,fill=False,linewidth=2,ax=g.ax_marg_x)\n",
    "sns.kdeplot(y=y, linewidth=2,ax=g.ax_marg_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes too much time and not that good visualisation\n",
    "g=sns.JointGrid()\n",
    "x, y = df_results_eval_test[df_results_eval_test['Incorrect']==0]['uncertainty'], df_results_eval_test['best_score']\n",
    "sns.scatterplot(x=x,y=y,ec='b',fc='none',s=100,linewidth=1.5,ax=g.ax_joint)\n",
    "sns.histplot(x=x,fill=False,linewidth=2,ax=g.ax_marg_x)\n",
    "sns.kdeplot(y=y, linewidth=2,ax=g.ax_marg_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='uncertainty', y='best_score', row='Incorrect', kind='box',\n",
    "            orient='v', height=4.5, aspect=4,\n",
    "           data=df_results_eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RUN_UNTIL_HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "metric = 'uncertainty'\n",
    "d_match = df_results_eval[df_results_eval['actual_class'] == df_results_eval['best_pred']][metric]\n",
    "d_match = d_match.astype(float)\n",
    "d_unmatch = df_results_eval[df_results_eval['actual_class'] != df_results_eval['best_pred']][metric]\n",
    "d_unmatch = d_unmatch.astype(float)\n",
    "# sns.displot(data=df_results_eval_test, x=\"uncertainty\", hue=\"Incorrect\", kind=\"kde\")\n",
    "# sns.displot(data=df_results_eval_test, x=\"uncertainty\", hue=\"Incorrect\", multiple=\"stack\", kind=\"kde\")\n",
    "# sns.histplot(df_results_eval_test, x=\"uncertainty\", hue=\"Incorrect\", element=\"poly\")\n",
    "#sns.histplot(df_results_eval_test, x=\"uncertainty\", hue=\"Incorrect\", element=\"step\", kde=True, stat='probability')\n",
    "sns.histplot(data=df_results_eval_test, x=\"uncertainty\", hue=\"Incorrect\", multiple=\"dodge\", shrink=.9, kde=True)\n",
    "#sns.displot(data=df_results_eval_test, x=\"uncertainty\", kde=True)\n",
    "# sns.displot(data=d_match, kind=\"kde\")\n",
    "# sns.displot(data=d_unmatch, kind=\"kde\")\n",
    "# sns.histplot(data=d_match, stat='density')\n",
    "#sns.histplot(data=d_unmatch, stat='density', color='y')\n",
    "# sns.displot(df_results_eval_test, x=metric, hue=\"Incorrect\", stat=\"density\", common_norm=False)\n",
    "# sns.displot(df_results_eval_test, x=metric, hue=\"Incorrect\", stat=\"probability\",\n",
    "#             common_norm=False,aspect=2, height=5)\n",
    "#sns.displot(df_results_eval_test, x=metric, hue=\"Incorrect\", kind=\"kde\",common_norm=False,aspect=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_similarities(n_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "line1, = ax1.plot(list(df_Softmax['fail_rate']), color='blue', lw=2)\n",
    "line2, = ax2.plot(list(df_Softmax['work_load']), color='blue', lw=2)\n",
    "\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_results_eval_test, x=metric, hue=\"Incorrect\", kind=\"kde\", common_norm=False, aspect=2, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_histogram(metric = 'uncertainty', kde=True):\n",
    "    d_match = df_results_eval[df_results_eval['actual_class'] == df_results_eval['best_pred']][metric]\n",
    "    d_unmatch = df_results_eval[df_results_eval['actual_class'] != df_results_eval['best_pred']][metric]\n",
    "    sns.set_style('darkgrid')\n",
    "    sns.distplot(d_match, kde=kde)  # , rug=True\n",
    "    sns.distplot(d_unmatch, kde=kde, rug=True)\n",
    "    \n",
    "    print(f'min: {round(min(df_results_eval[metric]), 4)}, max: {max(df_results_eval[metric])}, \\\n",
    "          mean: {round(np.mean(df_results_eval[metric]),4)}, median: {np.median(df_results_eval[metric])}')\n",
    "    \n",
    "def plot_histogram_seaborn(data_frame, col_stat, col_type, is_stat=True, common_norm=False, aspect=2):\n",
    "    # height=5\n",
    "    if is_stat:\n",
    "        sns.displot(data_frame, x=col_stat, hue=col_type, common_norm=common_norm, \n",
    "                    aspect=aspect, stat=\"probability\")    \n",
    "    else:\n",
    "        sns.displot(data_frame, x=col_stat, hue=col_type, common_norm=common_norm, \n",
    "                    aspect=aspect, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results_eval_test.loc[:5,[d_stat, d_type]]\n",
    "np.min(df_results_eval_test['Incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_stat = 'uncertainty'\n",
    "d_type = 'Incorrect'\n",
    "plot_histogram_seaborn(df_results_eval_test, d_stat, d_type,common_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_seaborn(df_results_eval_test, d_stat, d_type, is_stat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df_results_eval_test, x=\"uncertainty\", y=\"best_score\", height=5, ratio=2, marginal_ticks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(\n",
    "    data=df_results_eval_test, x=\"uncertainty\", y=\"best_score\",\n",
    "    marker=\"+\", s=100, marginal_kws=dict(bins=25, fill=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.jointplot(data=df_results_eval_test, x=\"uncertainty\", y=\"best_score\", kind=\"hex\")\n",
    "# sns.jointplot(data=df_results_eval_test, x=\"uncertainty\", y=\"best_score\", kind=\"hist\")\n",
    "# sns.jointplot(data=df_results_eval_test, x=\"uncertainty\", y=\"best_score\", kind=\"reg\")\n",
    "sns.jointplot(data=df_results_eval_test, x=\"uncertainty\", y=\"best_score\", hue=\"Incorrect\", \n",
    "              kind=\"kde\", common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df_results_eval_test, x=\"uncertainty\", y=\"best_score\", hue=\"Incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_stat = 'best_score'\n",
    "d_type = 'Incorrect'\n",
    "plot_histogram_seaborn(df_results_eval_test, d_stat, d_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_seaborn(df_results_eval_test, d_stat, d_type, is_stat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(metric = 'uncertainty', kde=True)\n",
    "# sns.displot(df_results_eval_test, x='uncertainty', hue='Incorrect', \n",
    "#                     aspect=2, kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(metric = 'best_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(metric = 'u_entr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_results_eval_test, fit=stats.laplace, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_results_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.python-graph-gallery.com/25-histogram-with-several-variables-seaborn\n",
    "# libraries & dataset\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) \n",
    "sns.set(style=\"darkgrid\")\n",
    "df = sns.load_dataset(\"iris\")\n",
    "\n",
    "sns.histplot(data=df_results_eval, x=\"uncertainty\", color=\"skyblue\", label=\"Sepal Length\", kde=True)\n",
    "sns.histplot(data=df_results_eval, x=\"best_score\", color=\"red\", label=\"Sepal Width\", kde=True)\n",
    "\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((plt_sub1)) = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\n",
    "df_preds_rate = plot_metrics_test(plt_sub1, title='preds_rate', metric_name='preds_rate', step_size=0.01, higher_better=True, is_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flexible = df_preds_rate[(df_preds_rate['work_load']<=5) & (df_preds_rate['fail_rate']<=1)] \n",
    "lst_flexible = list(df_flexible['val'])\n",
    "lst_flexible_workload = list(df_flexible['work_load'])\n",
    "print(f'{len(lst_flexible)} values from {min(lst_flexible)} to {max(lst_flexible)} having fail rate between 0.5 and 1.0')\n",
    "print(f'workload from {min(lst_flexible_workload)} to {max(lst_flexible_workload)}')\n",
    "\n",
    "print(3.753371-3.622185)\n",
    "print((3.753371-3.622185)/3.622185*100)\n",
    "\n",
    "df_flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_rate[df_preds_rate['fail_rate']<=3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "244/13721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((plt_sub1)) = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\n",
    "plot_metrics(plt_sub1, title='Entr', metric_name='entr', step_size=0.01, higher_better=False, is_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STD (Horizontal - Between classes [after averaging the results for the ensembles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_metrics(title='std', metric_name='std', min_max_step=(0.9, 0.99, 0.01), is_norm=False)\n",
    "fig, ((plt_sub1)) = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\n",
    "plot_metrics(plt_sub1, title='std', metric_name='std', step_size=0.001, higher_better=True, is_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuc1.df_result_ensemble[:1]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "c_list = ['picture','uncertainty', 'actual_class', 'best_pred','best_score', 'best_2nd_score', 'P_jsd', 'P_info', 'std', 'u_entr', 'var_Top1']\n",
    "df_results_eval[df_results_eval['actual_class'] != df_results_eval['best_pred']][c_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval[df_results_eval['actual_class'] == df_results_eval['best_pred']][c_list][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble[tuc1.df_result_ensemble['picture'] == 'MA160814197500041.070-K_4_0_.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100-np.mean(df_results_eval.loc[:, 'Certainty']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100-np.mean(df_results_eval[df_results_eval['actual_class']==df_results_eval['best_pred']]['Certainty']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100-np.mean(df_results_eval[df_results_eval['actual_class']!=df_results_eval['best_pred']]['Certainty']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(1-np.mean(df_results_eval.loc[:, 'best_score']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(1-np.mean(df_results_eval[df_results_eval['actual_class']==df_results_eval['best_pred']]['best_score']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(1-np.mean(df_results_eval[df_results_eval['actual_class']!=df_results_eval['best_pred']]['best_score']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(df_results_eval.loc[:, 'u_entr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(df_results_eval.loc[:, 'u_entr']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(df_results_eval[df_results_eval['actual_class']==df_results_eval['best_pred']]['u_entr']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(df_results_eval[df_results_eval['actual_class']!=df_results_eval['best_pred']]['u_entr']), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# set a grey background (use sns.set_theme() if seaborn version 0.11.0 or above) \n",
    "sns.set(style=\"darkgrid\")\n",
    "df = sns.load_dataset(\"iris\")\n",
    "\n",
    "sns.histplot(data=df_results_eval, x=\"Certainty\", color=\"skyblue\", label=\"Sepal Length\", kde=True)\n",
    "sns.histplot(data=df_results_eval, x=\"best_score\", color=\"red\", label=\"Sepal Width\", kde=True)\n",
    "\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.loc[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# xx=np.array([1,2,3,4,0])\n",
    "xx=np.array([1/5,1/5,1/5,1/5,1/5])\n",
    "Counter(xx)\n",
    "# print(np.bincount(xx))\n",
    "# np.var(np.bincount(xx))\n",
    "# entr(np.bincount(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble[tuc1.df_result_ensemble['picture'] ==\n",
    "                        'MA160873676300027.057-K_20_1_.png']['outputs_all'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_results_eval[df_results_eval['actual_class'] == '1']['uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_results_eval['uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_results_eval[df_results_eval['actual_class'] == df_results_eval['best_pred']]['uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_results_eval[df_results_eval['actual_class'] != df_results_eval['best_pred']]['uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean(df_results_eval, metric = 'uncertainty'):\n",
    "    mean_all = round(np.mean(df_results_eval[metric]), 4)\n",
    "    mean_correct = round(np.mean(df_results_eval[df_results_eval['actual_class'] == \n",
    "                                           df_results_eval['best_pred']][metric]), 4)\n",
    "    mean_incorrect = round(np.mean(df_results_eval[df_results_eval['actual_class'] != \n",
    "                                             df_results_eval['best_pred']][metric]), 4)\n",
    "    \n",
    "    return {'mean_all': mean_all, 'mean_correct': mean_correct, 'mean_incorrect': mean_incorrect}    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_mean(df_results_eval, metric = 'uncertainty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_mean(df_results_eval, metric = 'Certainty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df_results_eval['uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval[df_results_eval['actual_class'] == '1']['uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.show_results(result = 'ALL') # mean=mean top-2 uncertainty of the given class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=np.array([1,2,8,4,5])\n",
    "np.argmax(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.show_results(result = 'match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.show_results(result = 'unmatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.show_results(result = 'combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "tuc1.show_results(result = 'ALL2nd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval[df_results_eval['actual_class'] != \n",
    "                df_results_eval['best_pred']].loc[:,['picture','actual_class', 'best_pred', 'Certainty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.loc[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_uncertainty.loc[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval[df_results_eval['actual_class'] == \n",
    "                df_results_eval['best_pred']].loc[:,['picture','actual_class', 'best_pred', 'Certainty']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "plt.figure()\n",
    "class_names = range(24)\n",
    "plot_confusion_matrix(confusion, classes=class_names, normalize=False,title='Normalized confusion matrix')\n",
    "# plot_confusion_matrix(confusion, normalize=True,title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "l_label = list(df_results_eval['actual_class'])\n",
    "l_pred = list(df_results_eval['best_pred'])\n",
    "performance.plot_confusion_matrix(l_label, l_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(l_label, l_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/multi-class-classification-extracting-performance-metrics-from-the-confusion-matrix-b379b427a872\n",
    "#https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826#:~:text=False%20Positive%20(FP)%3A%20It,the%20positive%20class%20as%20negative.\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum_TP)/(sum_TP+sum_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_TP = sum(TP)\n",
    "sum_TN = sum(TN)\n",
    "sum_FN = sum(FN)\n",
    "sum_FP = sum(FP)\n",
    "sum_FN/(sum_TP+sum_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum_TP+sum_TN)/(sum_TP+sum_FP+sum_FN+sum_TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_FN/(sum_TP+sum_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_label_idx = list(df_results_eval['class_idx'])\n",
    "l_pred_idx = list(df_results_eval['best_index'])\n",
    "l_predict_proba = list(df_results_eval['Mean'])\n",
    "roc_auc_score(l_label_idx, l_predict_proba, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "arr_label_idx = np.array(l_label_idx).reshape(-1,1)\n",
    "arr_predict_proba = np.array(l_predict_proba)\n",
    "\n",
    "#label_encoder = LabelEncoder()\n",
    "#l_label_int = label_encoder.fit_transform(l_label_int)    \n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(arr_label_idx)   # onehot_encoded => array(852,24)=(n_samples, n_classes)\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"ALL\"], tpr[\"ALL\"], _ = roc_curve(onehot_encoded.ravel(), arr_predict_proba.ravel())\n",
    "roc_auc[\"ALL\"] = auc(fpr[\"ALL\"], tpr[\"ALL\"])\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[\"ALL\"], tpr[\"ALL\"], color='darkorange',\n",
    "lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[\"ALL\"])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "fpr_ALL, tpr_ALL, roc_auc_val = fpr['ALL'], tpr['ALL'], roc_auc[\"ALL\"]\n",
    "# print(f\"fpr_ALL: {fpr_ALL} .. ttpr_ALL: {tpr_ALL} .. roc_auc_val: {roc_auc_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arr_label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics.confusion_matrix(l_label, l_pred))\n",
    "print(metrics.classification_report(l_label, l_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_label_idx = np.array(l_label_idx).reshape(-1,1)\n",
    "arr_predict_proba = np.array(l_predict_proba)\n",
    "\n",
    "# plt_class = for which class you want to plot the ROC CURVE\n",
    "fpr_micro, tpr_micro, roc_auc_val = \\\n",
    "performance.plot_roc_curve(arr_label_idx, arr_predict_proba, n_classes=24, plt_class=22)\n",
    "\n",
    "print(round(roc_auc_val,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_results_eval[df_results_eval['picture']=='MA160814197500042.017-K_9_0_.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_eval.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_eval\n",
    "# output = tuc1.softmax_and_reshape(tuc1.df_result_ensemble.loc[0,'outputs_all'])\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import entr\n",
    "# entr(output[:,8]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# entr_per_model=entr(output).sum(axis=1)/np.log(2)\n",
    "# entr_per_model2=entr(output).sum(axis=0)/np.log(2)\n",
    "# print(entr_per_model)\n",
    "# print(np.mean(entr_per_model))\n",
    "# print(entr_per_model2)\n",
    "# print(np.mean(entr_per_model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=0\n",
    "# # variance of the scores of the best prediction (pred with the highest score)\n",
    "# # indexes are preserved between 2 dataframes\n",
    "# np.var(tuc1.normalize_array_preprocess(tuc1.df_result_ensemble.loc[x,\n",
    "#                                         'outputs_all'])[:,df_results_eval.loc[x,'best_index']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_results_eval)):\n",
    "    if i %1000 == 0:\n",
    "        pic1 = df_results_eval.loc[i,'picture']\n",
    "        pic2 = tuc1.df_result_ensemble.loc[i,'picture']\n",
    "        print(f'df_results_eval: {pic1} ... df_result_ensemble: {pic2}')\n",
    "    if df_results_eval.loc[i,'picture'] != tuc1.df_result_ensemble.loc[i,'picture']:\n",
    "        print('mismatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[0,'outputs_all'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[:0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble['outputs_all'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "t_test=tuc1.softmax_and_reshape(tuc1.df_result_ensemble.loc[x, 'outputs_all'])[:,df_results_eval.loc[x,'best_index']]\n",
    "t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tuc1.df_result_ensemble.loc[0, 'outputs_all'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[0, 'outputs_all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[0, 'outputs_all'][:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "softmax(tuc1.df_result_ensemble.loc[0, 'outputs_all'][:,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "tuc1.normalize_array_preprocess(tuc1.df_result_ensemble.loc[x,'outputs_all'])[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.normalize_array_preprocess(tuc1.df_result_ensemble.loc[x,'outputs_all'])[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "np.argmax(tuc1.normalize_array_preprocess(tuc1.df_result_ensemble.loc[x,'outputs_all'])[1:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuc1.df_result_ensemble.loc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "np.argmax(tuc1.df_result_ensemble.loc[:,'outputs_all'][x][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
